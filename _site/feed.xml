<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SHL</title>
    <description>在学习DL性能优化的学生</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 04 Nov 2018 16:06:48 +0800</pubDate>
    <lastBuildDate>Sun, 04 Nov 2018 16:06:48 +0800</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>ARMv8常用指令</title>
        <description>&lt;h3 id=&quot;ld1&quot;&gt;LD1&lt;/h3&gt;

&lt;p&gt;ld1指令可以从内存中load数据到一个或多个寄存器
&lt;img src=&quot;/images/posts/2018-11-09-ARMv8/1.jpg&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;when opcode == 0111.
LD1 { &amp;lt;Vt&amp;gt;.&amp;lt;T&amp;gt; }, [&amp;lt;Xn|SP&amp;gt;]
when opcode == 1010.
LD1 { &amp;lt;Vt&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt2&amp;gt;.&amp;lt;T&amp;gt; }, [&amp;lt;Xn|SP&amp;gt;]
when opcode == 0110.
LD1 { &amp;lt;Vt&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt2&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt3&amp;gt;.&amp;lt;T&amp;gt; }, [&amp;lt;Xn|SP&amp;gt;]
when opcode == 0010.
LD1 { &amp;lt;Vt&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt2&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt3&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vt4&amp;gt;.&amp;lt;T&amp;gt; }, [&amp;lt;Xn|SP&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;T可以为以下值，设置T时还会指定size和Q位置的值：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;8B when size = 00,Q = 0&lt;/li&gt;
  &lt;li&gt;16B when size = 00,Q = 1&lt;/li&gt;
  &lt;li&gt;4H when size = 01,Q = 0&lt;/li&gt;
  &lt;li&gt;8H when size = 01,Q = 1&lt;/li&gt;
  &lt;li&gt;2S when size = 10,Q = 0&lt;/li&gt;
  &lt;li&gt;4S when size = 10,Q = 1&lt;/li&gt;
  &lt;li&gt;1D when size = 11,Q = 0&lt;/li&gt;
  &lt;li&gt;2D when size = 11,Q = 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Xn|SP&amp;gt;&lt;/code&gt; Is the 64-bit name of the general-purpose base register or stack pointer, encoded in the “Rn” field.&lt;/p&gt;

&lt;h3 id=&quot;ext&quot;&gt;EXT&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EXT &amp;lt;Vd&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vn&amp;gt;.&amp;lt;T&amp;gt;, &amp;lt;Vm&amp;gt;.&amp;lt;T&amp;gt;, #&amp;lt;index&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这应该是指令的存储在寄存器上的格式（encode了所有所需要的信息）。
&lt;img src=&quot;/images/posts/2018-11-09-ARMv8/2.jpg&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ext实现的功能应该类似于neon函数&lt;code class=&quot;highlighter-rouge&quot;&gt;vextq_f32&lt;/code&gt;，就是将前后两个寄存器里存的值组合起来放到一个寄存器中，由最后一位index来指定，不过有点让人难以看懂。&lt;/p&gt;

&lt;p&gt;vextq_f32例子如下。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float _a[] = {1,2,3,4}, _b[] = {5,6,7,8} ;
float32x4_t a = vld1q_f32(_a), b = vld1q_f32(_b);
float32x4_t r1 = vextq_f32(a,b,1); //r1={2,3,4,5}
float32x4_t r2 = vextq_f32(a,b,2); //r2={3,4,5,6}
float32x4_t r3 = vextq_f32(a,b,3); //r3={4,5,6,7}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ncnn中的使用例子如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;prfm       pldl1keep, [%4, #256]          \n&quot;
&quot;ld1        {v9.4s, v10.4s}, [%4]          \n&quot;// v9 v10 = r10 r14
&quot;ext        v11.16b, v9.16b, v10.16b, #4   \n&quot; //r11
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Vd, Vn, Vm&lt;/code&gt;即3个通用的寄存器，它们的信息分别被encode在&lt;code class=&quot;highlighter-rouge&quot;&gt;Rd, Rn, Rm&lt;/code&gt;中&lt;/p&gt;

&lt;p&gt;根据arm文档所示，T只会是8b或者16b（这里的b表示byte，字节）&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;T = 8b,  when Q=0&lt;/li&gt;
  &lt;li&gt;T = 16b, when Q=1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个Q应该要同上文ld1进来时的Q相匹配。
因为前面ld1指令指定了v9，v10寄存器为4s，即4个32位。所以Q=1，即T只能为16b。&lt;/p&gt;

&lt;p&gt;同时，&lt;code class=&quot;highlighter-rouge&quot;&gt;index&lt;/code&gt;的信息被encode在imm4中（index基于字节表示偏移 index is the lowest numbered byte element to be extracted），&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;imm4&amp;lt;2:0&amp;gt; when Q = 0, imm4&amp;lt;3&amp;gt; = 0&lt;/li&gt;
  &lt;li&gt;imm4 when Q = 1, imm4&amp;lt;3&amp;gt; = x
The encoding Q = 0, imm4&amp;lt;3&amp;gt; = 1 is reserved.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;也就是说，当Q=0时，imm4只有3位是有效的（即index范围为0～7），即imm4&amp;lt;3&amp;gt;=0。Q=0，imm4&amp;lt;3&amp;gt;=1时无定义
当Q=1时，imm4的4位都有效，即index范围为0～15。&lt;/p&gt;

&lt;p&gt;具体EXT实现的功能如下所示，下图每一格表示一个字节（8位），则其为64位，如果是Q=1时，则是16格&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-11-09-ARMv8/3.jpg&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Decode for this encoding
integer d = UInt(Rd); 			//Vd的位置被encode在Rd中
integer n = UInt(Rn); 			//同上
integer m = UInt(Rm);			//同上
if Q == '0' &amp;amp;&amp;amp; imm4&amp;lt;3&amp;gt; == '1' then UNDEFINED; 	//Q=0，imm4&amp;lt;3&amp;gt;=1时不存在
integer datasize = if Q == '1' then 128 else 64;	//Q=1时用整个128位寄存器，Q=0用64位寄存器
integer position = UInt(imm4) &amp;lt;&amp;lt; 3;				//偏移的位用imm4x8，即字节转位
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;fmlavector&quot;&gt;FMLA(vector)&lt;/h3&gt;

&lt;p&gt;Floating-point fused Multiply-Add to accumulator (vector).&lt;/p&gt;

&lt;h3 id=&quot;fmulvector&quot;&gt;FMUL(vector)&lt;/h3&gt;

&lt;p&gt;Floating-point Multiply (vector).&lt;/p&gt;

&lt;h3 id=&quot;以占位符方式访问向量寄存器&quot;&gt;以占位符方式访问向量寄存器&lt;/h3&gt;

&lt;p&gt;直接在后面加后缀来指明立场的；如浮点乘法的时候就是%16.4s指明是单精度浮点（4个single精度浮点值），同样的&lt;code class=&quot;highlighter-rouge&quot;&gt;v21.s[3]&lt;/code&gt;是访问4个中的其中一个浮点值。（引自ncnn）&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// v寄存器单路使用 %.s[0] %.s[1] %.s[2] %.s[3]
// a += b * c[0]
// a += b * c[1]
// a += b * c[2]
// a += b * c[3]
float32x4_t _a = vld1_f32(a);
float32x4_t _b = vld1_f32(b);
float32x4_t _c = vld1_f32(c);
asm volatile(
    &quot;fmla  %0.4s, %2.4s, %3.s[0]&quot;
    &quot;fmla  %0.4s, %2.4s, %3.s[1]&quot;
    &quot;fmla  %0.4s, %2.4s, %3.s[2]&quot;
    &quot;fmla  %0.4s, %2.4s, %3.s[3]&quot;
    : &quot;=w&quot;(_a) // %0
    : &quot;0&quot;(_a),
      &quot;w&quot;(_b), // %2
      &quot;w&quot;(_c)  // %3
    :
);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 03 Nov 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/11/ARMv8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/11/ARMv8/</guid>
        
        <category>ARM</category>
        
        
      </item>
    
      <item>
        <title>TextBoxes_plusplus的NNIE实现文档</title>
        <description>&lt;h3 id=&quot;模型转换&quot;&gt;模型转换&lt;/h3&gt;

&lt;p&gt;在执行代码之前，我们先要将caffe的模型转换为nnie的模型（目前nnie只支持caffe模型转换）。&lt;/p&gt;

&lt;p&gt;假设我们手上现在有tbpp的模型，即一个caffemodel文件和一个depoly.prototxt文件，现在要将模型转换为nnie的模型wk文件。&lt;font color=&quot;red&quot;&gt;下面是一些模型转换时需要注意的地方。&lt;/font&gt;&lt;/p&gt;

&lt;h4 id=&quot;nnie_mapper配置&quot;&gt;nnie_mapper配置&lt;/h4&gt;

&lt;p&gt;将二进制文件解压出来即可使用。&lt;/p&gt;

&lt;h4 id=&quot;prototxt文件配置&quot;&gt;prototxt文件配置&lt;/h4&gt;

&lt;p&gt;按照HiSVP开发指南的说法，prototxt需要遵循一定的格式。在这里我们只需要将需要CPU实现的层删除即可，这些层分别为&lt;font color=&quot;red&quot;&gt;PriorBox层、Softmax层和DetectionOutput层&lt;/font&gt;（虽然文档说似乎是支持的&lt;font color=&quot;red&quot;&gt;Flatten层&lt;/font&gt;的，但sample里是在CPU中实现的，所以也要删除）。
网络从前往后只保留到各个Permute层，Permute后面的层要全部删除。&lt;/p&gt;

&lt;p&gt;然后input层也需要遵循格式要求&lt;/p&gt;
&lt;h6 id=&quot;deployprototxt-输入层格式&quot;&gt;deploy.prototxt 输入层格式&lt;/h6&gt;
&lt;p&gt;deploy.prototxt 输入层支持如下两种格式，n 维度的 dim 值建议写 1，mapper 会根据参考图片路径中的图片张数自动生成 n 值: 
格式一:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input: &quot;data&quot;
input_shape{
   dim:1
   dim:3
   dim:224
   dim:224
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;格式二:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layer {
name: &quot;data&quot;
type: &quot;Input&quot;
top: &quot;data&quot;
input_param {
   shape: {
      dim: 1
      dim: 3
	  dim: 227
	  dim: 227 
    }
  } 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h6 id=&quot;中间上报层&quot;&gt;中间上报层&lt;/h6&gt;

&lt;p&gt;如果想要将某些中间层的结果抽取出来，可以使用&lt;code class=&quot;highlighter-rouge&quot;&gt;report&lt;/code&gt;关键词&lt;/p&gt;

&lt;p&gt;用户需要中间层结果输出时，需要对应层的&lt;code class=&quot;highlighter-rouge&quot;&gt;top&lt;/code&gt;域中添加&lt;code class=&quot;highlighter-rouge&quot;&gt;_report&lt;/code&gt;标识符进行标注。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;top 后续无节点，自然上报，_report 不增加上报点;&lt;/li&gt;
  &lt;li&gt;top 对应的后续节点有多个 bottom，且其中一个 bottom 是 cpu 层，则该 top 上报;&lt;/li&gt;
  &lt;li&gt;top 对应的后续节点是 cpu 层(其中，cpu 层指 proposal、custom、_cpu 层);&lt;/li&gt;
  &lt;li&gt;top 有后续节点，_report 增加上报点;&lt;/li&gt;
  &lt;li&gt;custom 有 top 加_report，报错;&lt;/li&gt;
  &lt;li&gt;proposal 有 top 加_report，不报错，也不增加上报点;&lt;/li&gt;
  &lt;li&gt;_cpu 有 top 加_report，不报错，也不增加上报点;&lt;/li&gt;
  &lt;li&gt;data 层加_report，不会报错，也不会增加上报点;&lt;/li&gt;
  &lt;li&gt;inplace 激活，_report 应加在 conv 层上，原因是多个激活共享了 conv 的 blob，因此这些层只输出一个 blob，加在激活层上不会报错，也不会增加上报点;&lt;/li&gt;
  &lt;li&gt;conv 加激活，如果用户想在 conv 层上报，必须把两个节点拆开(激活写成 non- inplace 方式，即激活的 top、bottom 不同名);&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;指定任意层高精度&quot;&gt;指定任意层高精度&lt;/h6&gt;

&lt;p&gt;用户指定自定义计算精度(compile_mode = 2)时，在对应层的层名后加上高精度&lt;code class=&quot;highlighter-rouge&quot;&gt;_hp&lt;/code&gt;(16 比特)标记，可实现指定任意层为高精度输入。格式如下所示&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layer {
    name: &quot;conv5_hp&quot;
    type: &quot;Convolution&quot;
    bottom: &quot;conv4&quot;
    top: &quot;conv5&quot;
    convolution_param {
       num_output: 256
       kernel_size: 3
       pad: 1
       stride: 1
	} 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;使用nnie_mapper生成模型&quot;&gt;使用nnie_mapper生成模型&lt;/h4&gt;

&lt;p&gt;nnie_mapper需要用户提供一个cfg文件，如下是常用cnn的基本配置（ssd也一样）。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[prototxt_file]  ./lenet.prototxt
[caffemodel_file] ./lenet_iter_10000.caffemodel
[batch_num] 0
[net_type] 0
[sparse_rate] 0
[compile_mode] 1
[is_simulation] 0
[log_level] 2
[instruction_name] ./lenet
[RGB_order] BGR
[data_scale] 0.0039062
[internal_stride] 16
[image_list] ./image_ref_list.txt
[image_type] 1
[mean_file] ./lenetmean.txt
[norm_type] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;具体的参数说明可以去文档中查看，这里只稍微介绍一些常常要改的重要参数。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[compile_mode]&lt;/code&gt;表示编译模式，默认为0，表示低精度高带宽。如果配置为1则是全网络高精度，这里的高精度其实也是有压缩的，是以16位int型计算的，如果配置为2的话则是部分层高精度，哪些层需要高精度需要用户自己配置，具体如何配置参照上面prototxt修改。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[image_list]&lt;/code&gt;为NNIE mapper 用于数据量化的参考图像 list 文件或 feature map 文件。
NNIE mapper 量化时需要的图片是典型场景图片，建议从网络模型 的测试场景随机选择 20~50 张作为参考图片进行量化，选择的图像要尽量覆盖模型的各个场景(比如检测人、车的模型，参考图像中必须由人、车，不能仅使用人或者无人无车的图像进行量化)。网络中如果存在多个输入层，则需要配置多个 image_list 项，顺 序、个数与 prototxt 完全对应。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[mean_file]&lt;/code&gt; 均值文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;配置好cfg文件后执行&lt;code class=&quot;highlighter-rouge&quot;&gt;nnie_mapper xxx.cfg&lt;/code&gt;即可&lt;/p&gt;

&lt;p&gt;模型转换基本的注意事项如上。&lt;/p&gt;

&lt;h3 id=&quot;sample代码修改&quot;&gt;sample代码修改&lt;/h3&gt;

&lt;p&gt;官方提供了ssd的sample代码，而TextBoxes_plusplus是基于ssd修改的，所以为了在NNIE上实现TextBoxes_plusplus，我基于官方的ssd代码进行了修改。&lt;/p&gt;

&lt;p&gt;基本可以参照之前TextBoxes_plusplus基于ncnn实现的文档。&lt;font color=&quot;red&quot;&gt;（注意内存的分配）&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;不过还是有些不同的地方，ncnn处只需要修改PriorBoxes和DetectionOutput层，而nnie处还有其他地方代码需要修改，具体如下。&lt;/p&gt;

&lt;p&gt;首先是&lt;code class=&quot;highlighter-rouge&quot;&gt;pstSoftWareParam&lt;/code&gt;参数初始化。&lt;/p&gt;

&lt;p&gt;NNIE输出的是一连串的数据，需要你自己来截断（换句话说，就是NNIE的输出是最原始的数据，除了数据，其他信息一点没有）
所以对于PriorBox层，需要提供各层的大小&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;au32PriorBoxWidth[0] = 48;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[1] = 24;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[2] = 12;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[3] = 6;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[4] = 4;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[5] = 2;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于softmax层和detecionout层需要提供输入的参数数量。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;au32SoftMaxInChn[0] = 92160;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[1] = 23040;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[2] = 5760;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[3] = 1440;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[4] = 640;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[5] = 160;

pstSoftWareParam-&amp;gt;au32DetectInputChn[0] = 552960;
pstSoftWareParam-&amp;gt;au32DetectInputChn[1] = 138240;
pstSoftWareParam-&amp;gt;au32DetectInputChn[2] = 34560;
pstSoftWareParam-&amp;gt;au32DetectInputChn[3] = 8640;
pstSoftWareParam-&amp;gt;au32DetectInputChn[4] = 3840;
pstSoftWareParam-&amp;gt;au32DetectInputChn[5] = 960;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;由于TextBoxes_plusplus的ratio数量同ssd不同，它有5个ratio，所以对应也需要全部修改。反正同priorbox层相关的数据都是需要修改的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;au32InputAspectRatioNum[0] = 4;
pstSoftWareParam-&amp;gt;af32PriorBoxAspectRatio[0][0] = 2;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;以上都需要自己手动计算。
之后是根据上述参数计算所需内存并进行分配。&lt;/p&gt;

&lt;h3 id=&quot;输入的格式&quot;&gt;输入的格式&lt;/h3&gt;

&lt;p&gt;输入的格式文档中并没有说，我是根据后缀这些猜出来的，确实也是如此。直接依次排序保存像素值，图片大小需要自己记录下来。&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Oct 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/10/NNIE_TextBoxes/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/NNIE_TextBoxes/</guid>
        
        <category>HiSi</category>
        
        
      </item>
    
      <item>
        <title>GDB调试</title>
        <description>&lt;p&gt;因为一般都是在linux命令行下直接编译代码，已经不太使用ide了，所以gdb这个调试神器也应该学习一下。
（一直用fprintf也实现显示不出专业性^_^）&lt;/p&gt;

&lt;p&gt;gdb调试其实就相当于另开了一个shell，在其中你可以输入各种命令来对程序进行操作。&lt;/p&gt;

&lt;h3 id=&quot;基础命令&quot;&gt;基础命令&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;run&lt;br /&gt;
缩写为 r ，表示执行程序（如果程序已经在执行了，则重新开始执行程序）。&lt;/li&gt;
  &lt;li&gt;continue&lt;br /&gt;
缩写为 c ，表示程序执行到下一个断点处停止。&lt;/li&gt;
  &lt;li&gt;break&lt;br /&gt;
缩写为 b ，设置断点。&lt;br /&gt;
用法：
    &lt;ul&gt;
      &lt;li&gt;b [行号]  -&amp;gt;  b 12&lt;/li&gt;
      &lt;li&gt;b [函数名]  -&amp;gt; b compute&lt;/li&gt;
      &lt;li&gt;b [条件]  -&amp;gt; b 7 if i==99&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;info&lt;br /&gt;
查看信息，用法为info [name] -&amp;gt; info b 表示查看断点信息。&lt;/li&gt;
  &lt;li&gt;step&lt;br /&gt;
缩写为 s ，表步进，如果当前执行的为函数，则进入函数内部。&lt;/li&gt;
  &lt;li&gt;next&lt;br /&gt;
缩写为 n ，单步执行，不会进入函数。&lt;/li&gt;
  &lt;li&gt;clear&lt;br /&gt;
用法：
    &lt;ul&gt;
      &lt;li&gt;clear 删除所有断点&lt;/li&gt;
      &lt;li&gt;clear [行号] 删除该行断点&lt;/li&gt;
      &lt;li&gt;clear [函数名] 删除该函数处断点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;quit&lt;br /&gt;
退出gdb调试&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;进阶命令&quot;&gt;进阶命令&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;watch&lt;br /&gt;
watch也是设置断点的一种方式，它可以跟踪变量，非常方便。（注意改变量在当前行应已被定义）
如 &lt;code class=&quot;highlighter-rouge&quot;&gt;watch i == 1&lt;/code&gt;，那么当&lt;code class=&quot;highlighter-rouge&quot;&gt;i=1&lt;/code&gt;时程序就会自动停下。&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 29 Oct 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/10/gdb/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/gdb/</guid>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>NNIE</title>
        <description>&lt;p&gt;NNIE 是 Neural Network Inference Engine 的简称，是海思媒体 SoC 中专门针对神经网 络特别是深度学习卷积神经网络进行加速处理的硬件单元，支持现有大部分的公开网 络，如 Alexnet、VGG16、Googlenet、Resnet18、Resnet50 等分类网络，Faster R- CNN、YOLO、SSD、RFCN 等检测网络，以及 SegNet、FCN 等场景分割网络。目前 NNIE 配套软件及工具链仅支持以 Caffe 框架，使用其他框架的网络模型需要转化 为 Caffe 框架下的模型。&lt;/p&gt;

&lt;h3 id=&quot;linux下模型转换工具nnie_mapper&quot;&gt;linux下模型转换工具nnie_mapper&lt;/h3&gt;

&lt;p&gt;通过设置不同的模式，&lt;code class=&quot;highlighter-rouge&quot;&gt;mapper&lt;/code&gt; 将 &lt;code class=&quot;highlighter-rouge&quot;&gt;*.caffemodel&lt;/code&gt; 转化成在仿真器、仿真库或板端上可加载执行的数据指令文件。&lt;/p&gt;

&lt;h3 id=&quot;量化方法&quot;&gt;量化方法&lt;/h3&gt;

&lt;p&gt;猜测:&lt;br /&gt;
应该是根据所提供的图片数据来进行分析，用的是非线性量化，应该是针对典型场景图片所有的像素值，将密集出现的像素值划区域压缩在一个值上。
应该跟输入的大小有关，用压缩过的图片来进行量化。&lt;/p&gt;

&lt;h3 id=&quot;信息存储结构体&quot;&gt;信息存储结构体&lt;/h3&gt;

&lt;p&gt;NNIE中定义了很多的结构体，并用这些结构体来存储信息，如果光看代码，很容易看着看着忘了上面的，所以记录下来。&lt;/p&gt;

&lt;h4 id=&quot;sample_svp_nnie_param_s-pstnnieparam&quot;&gt;SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam&lt;/h4&gt;

&lt;p&gt;这是NNIE中最重要的结构体，它保存了最关键的信息，包括网络参数，输入输出等。&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cm&quot;&gt;/*NNIE Execution parameters */&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hiSAMPLE_SVP_NNIE_PARAM_S&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;SVP_NNIE_MODEL_S&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;pstModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_U32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u32TmpBufSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_U32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;au32TaskBufSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;SVP_MEM_INFO_S&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;stTaskBuf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;SVP_MEM_INFO_S&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;stTmpBuf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;SVP_MEM_INFO_S&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;stStepBuf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//store Lstm step info
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;SAMPLE_SVP_NNIE_SEG_DATA_S&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;astSegData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//each seg's input and output blob
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;SVP_NNIE_FORWARD_CTRL_S&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;astForwardCtrl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;SVP_NNIE_FORWARD_WITHBBOX_CTRL_S&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;astForwardWithBboxCtrl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SAMPLE_SVP_NNIE_PARAM_S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pstModel&lt;/code&gt;是读取进来的模型&lt;code class=&quot;highlighter-rouge&quot;&gt;.wk&lt;/code&gt;文件&lt;/p&gt;

&lt;h6 id=&quot;sample_svp_nnie_seg_data_s-astsegdatasvp_nnie_max_net_seg_num&quot;&gt;SAMPLE_SVP_NNIE_SEG_DATA_S astSegData[SVP_NNIE_MAX_NET_SEG_NUM]&lt;/h6&gt;

&lt;p&gt;该结构体嵌套上结构体内，用于保存输入输出的信息。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/*each seg input and output memory*/
typedef struct hiSAMPLE_SVP_NNIE_SEG_DATA_S
{
	SVP_SRC_BLOB_S astSrc[SVP_NNIE_MAX_INPUT_NUM];
	SVP_DST_BLOB_S astDst[SVP_NNIE_MAX_OUTPUT_NUM];
}SAMPLE_SVP_NNIE_SEG_DATA_S;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;astSrc&lt;/code&gt;保存的是输入数据
&lt;code class=&quot;highlighter-rouge&quot;&gt;astDst&lt;/code&gt;保存的是输出数据&lt;/p&gt;

&lt;h4 id=&quot;sample_svp_nnie_cfg_s-pstnniecfg&quot;&gt;SAMPLE_SVP_NNIE_CFG_S *pstNnieCfg&lt;/h4&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cm&quot;&gt;/*NNIE configuration parameter*/&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hiSAMPLE_SVP_NNIE_CFG_S&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_CHAR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pszPic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_U32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u32MaxInputNum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_U32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u32MaxRoiNum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HI_U64&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;au64StepVirAddr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SAMPLE_SVP_NNIE_EACH_SEG_STEP_ADDR_NUM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//virtual addr of LSTM's or RNN's step buffer
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;SVP_NNIE_ID_E&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;aenNnieCoreId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVP_NNIE_MAX_NET_SEG_NUM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SAMPLE_SVP_NNIE_CFG_S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于ssd来说，这个结构体中只有&lt;code class=&quot;highlighter-rouge&quot;&gt;pszPic&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;aenNnieCoreId&lt;/code&gt;是有用的。&lt;/p&gt;

&lt;h4 id=&quot;sample_svp_nnie_ssd_software_param_s-pstsoftwareparam&quot;&gt;SAMPLE_SVP_NNIE_SSD_SOFTWARE_PARAM_S* pstSoftwareParam&lt;/h4&gt;

&lt;p&gt;ssd所需参数保存的结构体。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/*SSD software parameter*/
typedef struct hiSAMPLE_SVP_NNIE_SSD_SOFTWARE_PARAM_S
{
	/*----------------- Model Parameters ---------------*/
	HI_U32 au32ConvHeight[12];
	HI_U32 au32ConvWidth[12];
	HI_U32 au32ConvChannel[12];
	/*----------------- PriorBox Parameters ---------------*/
	HI_U32 au32PriorBoxWidth[6];
	HI_U32 au32PriorBoxHeight[6];
	HI_FLOAT af32PriorBoxMinSize[6][1];
	HI_FLOAT af32PriorBoxMaxSize[6][1];
	HI_U32 u32MinSizeNum;
	HI_U32 u32MaxSizeNum;
	HI_U32 u32OriImHeight;
	HI_U32 u32OriImWidth;
	HI_U32 au32InputAspectRatioNum[6];
	HI_FLOAT af32PriorBoxAspectRatio[6][2];
	HI_FLOAT af32PriorBoxStepWidth[6];
	HI_FLOAT af32PriorBoxStepHeight[6];
	HI_FLOAT f32Offset;
	HI_BOOL bFlip;
	HI_BOOL bClip;
	HI_S32 as32PriorBoxVar[4];
	/*----------------- Softmax Parameters ---------------*/
	HI_U32 au32SoftMaxInChn[6];
	HI_U32 u32SoftMaxInHeight;
	HI_U32 u32ConcatNum;
	HI_U32 u32SoftMaxOutWidth;
	HI_U32 u32SoftMaxOutHeight;
	HI_U32 u32SoftMaxOutChn;
	/*----------------- DetectionOut Parameters ---------------*/
	HI_U32 u32ClassNum;
	HI_U32 u32TopK;
	HI_U32 u32KeepTopK;
	HI_U32 u32NmsThresh;
	HI_U32 u32ConfThresh;
	HI_U32 au32DetectInputChn[6];
	HI_U32 au32ConvStride[6];
	SVP_MEM_INFO_S stPriorBoxTmpBuf;
	SVP_MEM_INFO_S stSoftMaxTmpBuf;
	SVP_DST_BLOB_S stClassRoiNum;
	SVP_DST_BLOB_S stDstRoi;
	SVP_DST_BLOB_S stDstScore;
	SVP_MEM_INFO_S stGetResultTmpBuf;
}SAMPLE_SVP_NNIE_SSD_SOFTWARE_PARAM_S;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;sample_comm_svp_checksysinit系统初始化&quot;&gt;SAMPLE_COMM_SVP_CheckSysInit系统初始化&lt;/h3&gt;

&lt;p&gt;具体内容资料太少无法理解，只能知道是用于初始化系统。&lt;/p&gt;

&lt;h3 id=&quot;sample_svp_nnie_ssd_paraminit-用于初始化ssd需要的参数&quot;&gt;SAMPLE_SVP_NNIE_Ssd_ParamInit 用于初始化ssd需要的参数&lt;/h3&gt;

&lt;p&gt;其中分别初始化硬件参数和软件参数。硬件参数指的是NNIE的参数，软件参数指的是实现NNIE不支持的层所需要的参数。&lt;/p&gt;

&lt;h4 id=&quot;硬件参数初始化&quot;&gt;硬件参数初始化&lt;/h4&gt;

&lt;p&gt;硬件参数放在&lt;code class=&quot;highlighter-rouge&quot;&gt;SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam&lt;/code&gt;中&lt;/p&gt;

&lt;h6 id=&quot;sample_svp_nnie_fillforwardinfo&quot;&gt;SAMPLE_SVP_NNIE_FillForwardInfo&lt;/h6&gt;

&lt;p&gt;因为我们这里用的是SSD的例子，所以填充的是参数&lt;code class=&quot;highlighter-rouge&quot;&gt;astForwardCtrl&lt;/code&gt;。
猜测 : 参数&lt;code class=&quot;highlighter-rouge&quot;&gt;astForwardCtrl&lt;/code&gt;只是一个备用，留档。除了NnieCoreId是从其他地方读取的，其余参数都是自己给自己赋值。&lt;/p&gt;

&lt;p&gt;同时用读取到的&lt;code class=&quot;highlighter-rouge&quot;&gt;pstModel&lt;/code&gt;的参数填充了&lt;code class=&quot;highlighter-rouge&quot;&gt;astSegData&lt;/code&gt;的参数。&lt;/p&gt;

&lt;h6 id=&quot;sample_svp_nnie_gettaskandblobbufsize&quot;&gt;SAMPLE_SVP_NNIE_GetTaskAndBlobBufSize&lt;/h6&gt;

&lt;p&gt;顾名思义，这个函数就是用来获取&lt;code class=&quot;highlighter-rouge&quot;&gt;task&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;blob&lt;/code&gt;所需的大小
得到所需大小后再根据其分配内存。&lt;/p&gt;

&lt;h4 id=&quot;软件参数初始化&quot;&gt;软件参数初始化&lt;/h4&gt;

&lt;p&gt;软件参数放在&lt;code class=&quot;highlighter-rouge&quot;&gt;SAMPLE_SVP_NNIE_SSD_SOFTWARE_PARAM_S* pstSoftWareParam&lt;/code&gt;中&lt;/p&gt;

&lt;h6 id=&quot;sample_svp_nnie_ssd_softwareinit&quot;&gt;SAMPLE_SVP_NNIE_Ssd_SoftwareInit&lt;/h6&gt;

&lt;p&gt;该函数用来初始化Ssd需要的参数。&lt;/p&gt;

&lt;p&gt;需要了解的是&lt;br /&gt;
NNIE输出的是一连串的数据，需要你自己来截断（换句话说，就是NNIE的输出是最原始的数据，除了数据，其他信息一点没有）&lt;/p&gt;

&lt;p&gt;所以对于PriorBox层，需要提供各层的大小&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;au32PriorBoxWidth[0] = 48;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[1] = 24;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[2] = 12;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[3] = 6;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[4] = 4;
pstSoftWareParam-&amp;gt;au32PriorBoxWidth[5] = 2;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于softmax层和detecionout层需要提供输入的参数数量。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;au32SoftMaxInChn[0] = 92160;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[1] = 23040;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[2] = 5760;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[3] = 1440;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[4] = 640;
pstSoftWareParam-&amp;gt;au32SoftMaxInChn[5] = 160;

pstSoftWareParam-&amp;gt;au32DetectInputChn[0] = 552960;
pstSoftWareParam-&amp;gt;au32DetectInputChn[1] = 138240;
pstSoftWareParam-&amp;gt;au32DetectInputChn[2] = 34560;
pstSoftWareParam-&amp;gt;au32DetectInputChn[3] = 8640;
pstSoftWareParam-&amp;gt;au32DetectInputChn[4] = 3840;
pstSoftWareParam-&amp;gt;au32DetectInputChn[5] = 960;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;以上都需要自己手动计算。&lt;/p&gt;

&lt;p&gt;之后是根据上述参数计算所需内存并进行分配。&lt;/p&gt;

&lt;h3 id=&quot;sample_svp_nnie_fillsrcdata读取图片数据&quot;&gt;SAMPLE_SVP_NNIE_FillSrcData读取图片数据&lt;/h3&gt;

&lt;p&gt;就是很简单的读入数据，每次读一行，因为地址要对齐，所以地址每次需要&lt;code class=&quot;highlighter-rouge&quot;&gt;+u32Stride&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; s32Ret = fread(pu8PicAddr,u32Width*u32VarSize,1,fp);
 pu8PicAddr += u32Stride;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;sample_svp_nnie_ssd_getresult代码实现nnie不支持的层&quot;&gt;SAMPLE_SVP_NNIE_Ssd_GetResult代码实现nnie不支持的层&lt;/h3&gt;

&lt;p&gt;最重要的部分，这里在cpu上实现了ssd需要的但nnie不支持的层（priorbox层、softmax层和detecionout层）。&lt;/p&gt;

&lt;h3 id=&quot;nnie大致流程以ssd为例&quot;&gt;nnie大致流程（以ssd为例）&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;先对系统进行初始化（这一步目前只能按照sample给的例子来实现）。&lt;/li&gt;
  &lt;li&gt;载入模型。&lt;/li&gt;
  &lt;li&gt;再进行参数初始化，分别初始化硬件和软件的参数。硬件参数为NNIE所需要的参数，如内存分配地址，网络分割出来的输出数量等；软件参数为后续代码实现的层所需要的参数，在ssd中为priorbox、softmax和detectionout层所需要的参数。&lt;/li&gt;
  &lt;li&gt;然后读取二进制（bgr）图片数据。减均值和归一化操作似乎集成在nnie中了，这里只需要读取数据即可。&lt;/li&gt;
  &lt;li&gt;接着将数据送入NNIE进行forward。&lt;/li&gt;
  &lt;li&gt;NNIEforward结束后会返回最后的几个层（如果你有设置中间上报层的话它也会返回）。&lt;/li&gt;
  &lt;li&gt;得到这几层的数据后就可以着手处理了，手写priorbox层、softmax层和detecionout层来得到最后的结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;网络切分&quot;&gt;网络切分&lt;/h3&gt;

&lt;p&gt;当网络中存在 Non-support 层时，需要将网络进行切分，不支持的部分由用户使用 CPU 或者 DSP 等方式实现，统称为非 NNIE 方式。由此整个网络会出现 NNIE-&amp;gt;非 NNIE- &amp;gt;NNIE… 的分段执行方式。&lt;/p&gt;

&lt;p&gt;nnie_mapper 将 NNIE 的 Non-support 层分为两种，“Proposal”层和“Custom”层:&lt;/p&gt;

&lt;h3 id=&quot;一些问题和使用细节&quot;&gt;一些问题和使用细节&lt;/h3&gt;

&lt;p&gt;FlushCache应该是用来将虚拟地址上的数据映射到物理地址上（Hi3559A的NNIE硬件模块的物理地址和代码的虚拟地址应该是不对应的），所以每次参数变化时都需要FlushCache。&lt;/p&gt;

&lt;p&gt;Permute层目前只支持(0,1,2,3)-&amp;gt;(0,2,3,1)&lt;/p&gt;

&lt;p&gt;Reshape层的第一维必须设为0，因为它只支持对C/H/W进行reshape。第一维设为0表示其与bottom一致。&lt;/p&gt;

&lt;p&gt;Slice层只支持分割C/H/W维度，所以不能设置axis=0（在rcnn的模型转换中，由于前置层Permute将timestep转到第一维，导致了Slice切分失败）&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pstNnieParam-&amp;gt;astSegData[0].astDst[i].u64VirAddr;&lt;/code&gt;就是第i个输出（如ssd中nnie有12个输出）。目前还不知道astSegData的num代表的意义。&lt;/p&gt;

&lt;p&gt;在输出时，report层是第一个输出的，然后才是没有后续层的输出。也就是说如果有report层，那么&lt;code class=&quot;highlighter-rouge&quot;&gt;pstNnieParam-&amp;gt;astSegData[0].astDst[0].u64VirAddr;&lt;/code&gt;必然是report层。&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Oct 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/10/NNIE/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/NNIE/</guid>
        
        <category>HiSi</category>
        
        
      </item>
    
      <item>
        <title>DockerFile</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;前面已经介绍了DockerFile的几个优点了，其实主要的优点就是它有一个系统的步骤，可以让别人很容易理解，并且很容易知道在基本映像中改变了什么确切的配置。&lt;/p&gt;

&lt;h3 id=&quot;dockerfile编写&quot;&gt;DockerFile编写&lt;/h3&gt;

&lt;p&gt;DockerFile其实就是一种被docker解释的脚本，由多条指令组成，每条指令对应linux下的指令。&lt;/p&gt;

&lt;p&gt;Dockerfile的指令是忽略大小写的，建议使用大写，使用#作为注释，每一行只支持一条指令，每条指令可以携带多个参数。&lt;/p&gt;

&lt;p&gt;常用命令如下。&lt;/p&gt;

&lt;h4 id=&quot;from指定初始镜像&quot;&gt;FROM（指定初始镜像）&lt;/h4&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;FROM &amp;lt;image&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;一般从官方仓库直接拉取。&lt;/p&gt;

&lt;h4 id=&quot;maintainer制定镜像制作者信息&quot;&gt;MAINTAINER（制定镜像制作者信息）&lt;/h4&gt;

&lt;p&gt;将镜像的制作者相关的信息写入到镜像中。&lt;/p&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;MAINTAINER &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;run调用命令&quot;&gt;RUN（调用命令）&lt;/h4&gt;

&lt;p&gt;RUN可以运行任何被基础镜像支持的命令。如基础镜像选择了ubuntu，那么软件管理能使用ubuntu的命令。&lt;/p&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;RUN &amp;lt;shell command&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;env设置环境变量&quot;&gt;ENV（设置环境变量）&lt;/h4&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;ENV &amp;lt;key&amp;gt; &amp;lt;value&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;add添加文件&quot;&gt;ADD（添加文件）&lt;/h4&gt;

&lt;p&gt;将主机的文件添加至镜像中，一般用来替换镜像源。&lt;/p&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;ADD &amp;lt;src&amp;gt; &amp;lt;dst&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;workdir切换目录&quot;&gt;WORKDIR（切换目录）&lt;/h4&gt;

&lt;p&gt;用法：&lt;code class=&quot;highlighter-rouge&quot;&gt;WORKDIR &amp;lt;dir&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;dockerfile用docker-build命令来构建镜像&quot;&gt;DockerFile用docker build命令来构建镜像&lt;/h3&gt;

&lt;p&gt;docker build 命令用于使用 Dockerfile 创建镜像。&lt;/p&gt;

&lt;p&gt;一般常用：
&lt;code class=&quot;highlighter-rouge&quot;&gt;docker build -t &amp;lt;name&amp;gt; DockerFilePath&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;语法
&lt;code class=&quot;highlighter-rouge&quot;&gt;docker build [OPTIONS] PATH | URL | -&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;OPTIONS说明：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--build-arg=[] :设置镜像创建时的变量；

--cpu-shares :设置 cpu 使用权重；

--cpu-period :限制 CPU CFS周期；

--cpu-quota :限制 CPU CFS配额；

--cpuset-cpus :指定使用的CPU id；

--cpuset-mems :指定使用的内存 id；

--disable-content-trust :忽略校验，默认开启；

-f :指定要使用的Dockerfile路径；

--force-rm :设置镜像过程中删除中间容器；

--isolation :使用容器隔离技术；

--label=[] :设置镜像使用的元数据；

-m :设置内存最大值；

--memory-swap :设置Swap的最大值为内存+swap，&quot;-1&quot;表示不限swap；

--no-cache :创建镜像的过程不使用缓存；

--pull :尝试去更新镜像的新版本；

--quiet, -q :安静模式，成功后只输出镜像 ID；

--rm :设置镜像成功后删除中间容器；

--shm-size :设置/dev/shm的大小，默认值是64M；

--ulimit :Ulimit配置。

--tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。

--network: 默认 default。在构建期间设置RUN指令的网络模式
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;textboxes_plusplus的cpu版dockerfile&quot;&gt;TextBoxes_plusplus的CPU版DockerFile&lt;/h3&gt;

&lt;p&gt;因为TextBoxes_plusplus没有提供CPU版本的DockerFile，而我的mac又不支持GPU，所以根据其提供的GPU版本略微修改了DockerFile。（修改了镜像源以加速配置）&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:14.04
MAINTAINER caffe-maint@googlegroups.com

RUN mv /etc/apt/sources.list /etc/apt/sources-bak.list
&lt;span class=&quot;c&quot;&gt;# sources.list在当前目录下，里面是修改的阿里云镜像源&lt;/span&gt;
ADD sources.list /etc/apt/sources.list

RUN apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y --no-install-recommends &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        build-essential &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        cmake &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        git &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        wget &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libatlas-base-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libboost-all-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libgflags-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libgoogle-glog-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libhdf5-serial-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libleveldb-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        liblmdb-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libopencv-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libprotobuf-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        libsnappy-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    libgeos-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        protobuf-compiler &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        python-dev &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        python-numpy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        python-pip &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        python-scipy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    python-opencv &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    rm -rf /var/lib/apt/lists/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 同上配置阿里云源&lt;/span&gt;
ADD pip.conf /root/.pip/pip.conf

ENV &lt;span class=&quot;nv&quot;&gt;CAFFE_ROOT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/caffe
WORKDIR &lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# FIXME: clone a specific git tag and use ARG instead of ENV once DockerHub supports this.&lt;/span&gt;
ENV &lt;span class=&quot;nv&quot;&gt;CLONE_TAG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;master

ARG CLONE_REPO

RUN git clone -b &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLONE_TAG&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; --depth 1 &lt;span class=&quot;nv&quot;&gt;$CLONE_REPO&lt;/span&gt; .

RUN &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;req &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;cat python/requirements.txt&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; pydot; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;pip install &lt;span class=&quot;nv&quot;&gt;$req&lt;/span&gt;; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    mkdir build &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;build &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    cmake -DCPU_ONLY&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 .. &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    make -j&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;nproc&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

RUN ln -s /dev/null /dev/raw1394

ENV PYCAFFE_ROOT &lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/python
ENV PYTHONPATH &lt;span class=&quot;nv&quot;&gt;$PYCAFFE_ROOT&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$PYTHONPATH&lt;/span&gt;
ENV PATH &lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/build/tools:&lt;span class=&quot;nv&quot;&gt;$PYCAFFE_ROOT&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;
RUN &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/build/lib&quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/ld.so.conf.d/caffe.conf &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ldconfig

WORKDIR /workspace
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;使用方法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;docker build -t tbpp:cpu --build-arg CLONE_REPO=$(git remote get-url --all origin) DockerFilePATH&lt;/code&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Sep 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/09/dockerfile/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/dockerfile/</guid>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>Docker使用</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;最近了解到了docker，才发现原来世界上还有这么好用的东西。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Docker是一个开放源代码软件项目，让应用程序布署在软件容器下的工作可以自动化进行，借此在Linux操作系统上，提供一个额外的软件抽象层，以及操作系统层虚拟化的自动管理机制。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;docker的方便之处&quot;&gt;Docker的方便之处&lt;/h3&gt;

&lt;p&gt;从事linux嵌入式开发或者深度学习方向的研究人员往往都会遇到这样一个问题—-服务器的环境配置。linux嵌入式开发需要配置交叉编译所需要的工具，而深度学习研究有各种各样的框架，并且这些框架有概率是互不兼容的（比如说有些论文实现依托于cudnn5.1，而有些则需要更高的版本），总不能为了搭一个环境而舍弃另一个环境吧。&lt;/p&gt;

&lt;p&gt;当服务器环境越配越多，此时，Docker的优势就体现出来了，Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。&lt;/p&gt;

&lt;p&gt;并且，Docker在迁移时更加方便，它没有其他依赖，可以说自己就是一个微型系统。&lt;/p&gt;

&lt;h3 id=&quot;docker使用介绍&quot;&gt;Docker使用介绍&lt;/h3&gt;

&lt;p&gt;docker中有镜像(image)和容器(container)的概念，镜像就是指已经打包好的容器，可以被pull或push的，而容器则是你正在使用的，已经实例化的镜像。&lt;/p&gt;

&lt;p&gt;一般将镜像视为模版，不存放任何代码，只配置环境，代码使用volume挂载放入容器。(&lt;code class=&quot;highlighter-rouge&quot;&gt;run&lt;/code&gt;时用&lt;code class=&quot;highlighter-rouge&quot;&gt;-v&lt;/code&gt;来挂载)&lt;/p&gt;

&lt;h2 id=&quot;docker常用命令&quot;&gt;docker常用命令&lt;/h2&gt;

&lt;h4 id=&quot;docker-images&quot;&gt;docker images&lt;/h4&gt;

&lt;p&gt;显示已有镜像&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker images
REPOSITORY     TAG         IMAGE ID        CREATED           SIZE
ubuntu        16.04      52b10959e8aa    12 days ago         115MB
ubuntu        14.04      8789038981bc    12 days ago         188MB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;docker-rmi&quot;&gt;docker rmi&lt;/h4&gt;

&lt;p&gt;删除镜像&lt;/p&gt;

&lt;h4 id=&quot;docker-system-prune--a&quot;&gt;docker system prune -a&lt;/h4&gt;

&lt;p&gt;释放空间&lt;/p&gt;

&lt;h4 id=&quot;docker-ps--a&quot;&gt;docker ps -a&lt;/h4&gt;

&lt;p&gt;显示所有容器&lt;/p&gt;

&lt;h4 id=&quot;docker-start&quot;&gt;docker start&lt;/h4&gt;

&lt;p&gt;在退出容器后需要先start才可以进入。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker start nnie
nnie
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;docker-attach&quot;&gt;docker attach&lt;/h4&gt;

&lt;p&gt;容器开启后使用该命令进入容器。&lt;/p&gt;

&lt;h4 id=&quot;docker-run&quot;&gt;docker run&lt;/h4&gt;

&lt;p&gt;语法&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run [OPTIONS] IMAGE [COMMAND] [ARG...]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一般常用&lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -t -i -v /YOUR/PATH:/root/ ImageName /bin/bash&lt;/code&gt;
&lt;strong&gt;注意&lt;/strong&gt;，挂载时要使用绝对路径。&lt;/p&gt;

&lt;p&gt;OPTIONS说明：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项；

-d: 后台运行容器，并返回容器ID；

-i: 以交互模式运行容器，通常与 -t 同时使用；

-p: 端口映射，格式为：主机(宿主)端口:容器端口

-t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用；

--name=&quot;nginx-lb&quot;: 为容器指定一个名称；

--dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致；

--dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致；

-h &quot;mars&quot;: 指定容器的hostname；

-e username=&quot;ritchie&quot;: 设置环境变量；

--env-file=[]: 从指定文件读入环境变量；

--cpuset=&quot;0-2&quot; or --cpuset=&quot;0,1,2&quot;: 绑定容器到指定CPU运行；

-m :设置容器使用内存最大值；

--net=&quot;bridge&quot;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；

--link=[]: 添加链接到另一个容器；

--expose=[]: 开放一个端口或一组端口；
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;docker-build&quot;&gt;docker build&lt;/h4&gt;

&lt;p&gt;build指令是通过DockerFile来构建镜像的，具体使用下一篇介绍。&lt;/p&gt;

&lt;h3 id=&quot;docker常用构建方法&quot;&gt;Docker常用构建方法&lt;/h3&gt;

&lt;p&gt;docker hub一般都有许多镜像提供给你直接拉取，如果刚好有你需要的，只要直接pull就可以了。
比如说我需要一个caffe的环境，那么我直接在docker hub上找到&lt;a href=&quot;https://hub.docker.com/r/bvlc/caffe/&quot;&gt;caffe&lt;/a&gt;，然后根据所需要的tag直接拉取镜像就可以了。&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 这里我拉取的是caffe的cpu版本&lt;/span&gt;
docker pull bvlc/caffe:cpu

&lt;span class=&quot;c&quot;&gt;# 通过下面这行命令就可以通过拉取的镜像生成自己的容器，并进入到容器中了&lt;/span&gt;
docker run -it bvlc/caffe:cpu /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;不过主流的可能还是使用DockerFile来构建镜像，DockerFile的优点如下，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dockerfile是Docker镜像的自动化脚本&lt;/li&gt;
  &lt;li&gt;它具有简单的图像语法，并自动做许多更改，手动将需要更多的时间。&lt;/li&gt;
  &lt;li&gt;Dockerfile有一个系统的步骤，可以让别人很容易理解，并且很容易知道在基本映像中改变了什么确切的配置。&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 18 Sep 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/09/docker/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/docker/</guid>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>C++代码优化</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;  相对于其他简单的高级语言来说，C++可以更好地发挥硬件性能，但就算是相同的算法，代码写的不好，C++也不一定比其他语言就快很多。一般来说，没有一种简单的方法可以完美优化所有情况，所以优化只是尽可能接近最完美的情况。&lt;/p&gt;

&lt;p&gt;  一般优化过程中有两个标准：&lt;br /&gt;
-Principle of diminishing returns.  先从耗时耗力少且优化效果好的部分开始着手优化。&lt;br /&gt;
-Principle of diminishing portability.  先从跨平台通用的代码开始优化。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;h5 id=&quot;optimizing-c&quot;&gt;&lt;a href=&quot;https://en.wikibooks.org/wiki/Optimizing_C%2B%2B&quot;&gt;optimizing C++&lt;/a&gt;&lt;/h5&gt;
</description>
        <pubDate>Tue, 19 Jun 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/06/optimizing-cpp/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/06/optimizing-cpp/</guid>
        
        <category>性能优化</category>
        
        
      </item>
    
      <item>
        <title>optimization（一）</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;为什么需要做性能优化？因为深度学习需要大量的计算力，而嵌入式平台不像云端服务器平台一样有超额算力，嵌入式平台需要各种抠细节来充分发挥其性能以勉强提供计算能力。现有许多框架（仍在持续更新）提供给嵌入式平台加速，如&lt;a href=&quot;https://github.com/Tencent/ncnn&quot;&gt;ncnn&lt;/a&gt;，&lt;a href=&quot;https://github.com/ARM-software/ComputeLibrary&quot;&gt;ARM Compute Library&lt;/a&gt;等。&lt;/p&gt;

&lt;p&gt;一般CNN性能优化的方向有下面几种。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;算法层面的优化
    &lt;ul&gt;
      &lt;li&gt;模型优化&lt;/li&gt;
      &lt;li&gt;卷积计算优化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;系统层面的优化
    &lt;ul&gt;
      &lt;li&gt;代码冗余优化&lt;/li&gt;
      &lt;li&gt;内存优化&lt;/li&gt;
      &lt;li&gt;并行计算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;算法层面的优化需要太多的数学基础，对做工程的我们来说太难了，只能等业界大佬的论文。模型的优化现在很多大佬都在做，现在主要用mobilenet，shufflenet等轻量模型做主干特征提取网络，还有用剪枝的操作来减少模型参数。&lt;br /&gt;
卷积计算的优化主要有两种方法，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;im2col: 目前几乎所有的主流计算框架包括Caffe, MXNet等都实现了该方法。该方法把卷积变成矩阵和矩阵的乘法，然后通过各种BLAS库来计算，因为BLAS库优化的非常好，所以这个方法速度是比较快的。&lt;/li&gt;
  &lt;li&gt;winograd: 由于乘法和加法在硬件实现上的时间复杂度一般是不一样的，乘法运算所需的时间通常远大于加法所需的时间。因此，用廉价运算代替昂贵运算也是加速运算。winograd就是通过变换来用加法来替换部分乘法以达到优化增速的目的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于工程师而言，主要是针对系统级的优化，需要考虑&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;代码是否有冗余&lt;/li&gt;
  &lt;li&gt;代码是否缓存友好&lt;/li&gt;
  &lt;li&gt;内存重要还是速度重要&lt;/li&gt;
  &lt;li&gt;是否有多核可以利用&lt;/li&gt;
  &lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;im2colgemm&quot;&gt;im2col+gemm&lt;/h3&gt;

&lt;p&gt;我这里就先&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jun 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/06/optimization/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/06/optimization/</guid>
        
        <category>性能优化</category>
        
        
      </item>
    
      <item>
        <title>Winograd for CNN（二）</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;winograd通过用加法代替乘法来加速运算，其需要预设部分矩阵来帮助计算，但矩阵的选择也是各有千秋，具体要如何选择大家可以网上搜索。算法对于不同的矩阵有着不同的速度和效果。而现在也有许多不同的winograd算法实现，我这里主要基于&lt;code class=&quot;highlighter-rouge&quot;&gt;Fast Algorithms for Convolutional Neural Networks&lt;/code&gt;的&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2, 3\times 3)&lt;/script&gt;实现。&lt;/p&gt;

&lt;p&gt;我这里只是粗略的实现了一下效果，代码就写的很简陋(￣▽￣)。&lt;/p&gt;

&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;

&lt;p&gt;这里我设置&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
G=\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5&amp;-0.5&amp;0.5\\0&amp;0&amp;1\end{bmatrix} %]]&gt;&lt;/script&gt;,&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
B^T=\begin{bmatrix} 1 &amp; 0 &amp; -1&amp;0 \\ 0 &amp;1 &amp; 1&amp;0 \\ 0&amp;-1&amp;1&amp;0\\0&amp;1&amp;0&amp;-1\end{bmatrix} %]]&gt;&lt;/script&gt;,&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
A^T=\begin{bmatrix} 1&amp; \quad1 &amp;\quad1&amp;\quad0\\0&amp;\quad1&amp;-1&amp;-1\end{bmatrix} %]]&gt;&lt;/script&gt;。以&lt;script type=&quot;math/tex&quot;&gt;4\times 4&lt;/script&gt;的输入，&lt;script type=&quot;math/tex&quot;&gt;3\times 3&lt;/script&gt;的卷积核为例。（注意输入是要加padding的）&lt;/p&gt;

&lt;h2 id=&quot;gggt&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;GgG^T&lt;/script&gt;&lt;/h2&gt;

&lt;p&gt;这一步对于同一个卷积核的值来说是固定，因此是可以提前计算好的，是offline的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;float **U_2x2_3x3(float *kernel)
{
    float G[4][3] = { { 1,    0,    0},
                      {0.5,  0.5,  0.5},
                      {0.5, -0.5,  0.5},
                      { 0,    0,    1} };
    float GT[3][4] = { { 1,  0.5,  0.5,  0},
                       { 0,  0.5, -0.5,  0},
                       { 0,  0.5,  0.5,  1} };
    
    float G_g[4][3] = {0};
    for (int i = 0; i &amp;lt; 4; ++i) {
        for (int j = 0; j &amp;lt; 3; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 3; ++k) {
                temp += G[i][k] * kernel[k * 3 + j];
            }
            G_g[i][j] = temp;
        }
    }

    float **G_g_GT;
    G_g_GT = (float**)malloc(4 * sizeof(float*));
    //float G_g_GT[4][4] = {0};
    for (int i = 0; i &amp;lt; 4; ++i) {
        G_g_GT[i] = (float*)malloc(4 * sizeof(float));
        memset(G_g_GT[i], 0, 4 * sizeof(float));
        for (int j = 0; j &amp;lt; 4; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 3; ++k) {
                temp += G_g[i][k] * GT[k][j];
            }
            G_g_GT[i][j] = temp;
        }
    }

    return G_g_GT;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;btdb&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;B^TdB&lt;/script&gt;&lt;/h2&gt;

&lt;p&gt;因为我预设的输入是用一维数组来表示二维的矩阵，所以需要一个矩阵起始位置来推断分割的矩阵。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;float **V_2x2_3x3(float *input, int start)
{
    int BT[4][4] = { {1,  0, -1,  0},
                     {0,  1,  1,  0},
                     {0, -1,  1,  0},
                     {0,  1,  0, -1}};
    int B[4][4] =  { { 1,  0,  0,  0},
                     { 0,  1, -1,  1},
                     {-1,  1,  1,  0},
                     { 0,  0,  0, -1}};

    float BT_d[4][4] = {0};
    for (int i = 0; i &amp;lt; 4; ++i) {
        for (int j = 0; j &amp;lt; 4; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 4; ++k) {
                temp += BT[i][k] * input[k * 6 + j + start];
            }
            BT_d[i][j] = temp;
        }
    }

    float **BT_d_B;
    BT_d_B = (float**)malloc(4 * sizeof(float*));
    //float BT_d_B[4][4] = {0};
    for (int i = 0; i &amp;lt; 4; ++i) {
        BT_d_B[i] = (float*)malloc(4 * sizeof(float));
        memset(BT_d_B[i], 0, 4 * sizeof(float));
        for (int j = 0; j &amp;lt; 4; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 4; ++k) {
                temp += BT_d[i][k] * B[k][j];
            }
            BT_d_B[i][j] = temp;
        }
    }

    return BT_d_B;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;atuodot-va&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;A^T[U\odot V]A&lt;/script&gt;&lt;/h2&gt;

&lt;p&gt;这里输出的就是局部的最终结果。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;float **M_2x2_3x3(float **U, float **V)
{
    int AT[2][4] = { {1, 1,  1,  0},
                     {0, 1, -1, -1} };

    int A[4][2] = { {1,  0},
                    {1,  1},
                    {1, -1},
                    {0, -1} };

    float M[4][4] = {0};
    for (int i = 0; i &amp;lt; 4; ++i) {
        for (int j = 0; j &amp;lt; 4; ++j) {
            M[i][j] = U[i][j] * V[i][j];
        }
    }

    for (int l = 0; l &amp;lt; 4; ++l) {
        free(V[l]);
    }
    free(V);

    float AT_M[2][4] = {0};
    for (int i = 0; i &amp;lt; 2; ++i) {
        for (int j = 0; j &amp;lt; 4; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 4; ++k) {
                temp += AT[i][k] * M[k][j];
            }
            AT_M[i][j] = temp;
        }
    }

    float **AT_M_A;
    AT_M_A = (float**)malloc(2 * sizeof(float*));
    for (int i = 0; i &amp;lt; 2; ++i) {
        AT_M_A[i] = (float*)malloc(2 * sizeof(float));
        memset(AT_M_A[i], 0, 2 * sizeof(float));
        for (int j = 0; j &amp;lt; 2; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; 4; ++k) {
                temp += AT_M[i][k] * A[k][j];
            }
            AT_M_A[i][j] = temp;
        }
    }

    return AT_M_A;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;winograd&quot;&gt;Winograd&lt;/h2&gt;

&lt;p&gt;因为我这里举的输入是&lt;script type=&quot;math/tex&quot;&gt;4\times 4&lt;/script&gt;，卷积为&lt;script type=&quot;math/tex&quot;&gt;3\times 3&lt;/script&gt;的例子，所以局部结果可以刚好拼成最终结果，而且padding也只要上下左右各加1就可以了。但如果是&lt;script type=&quot;math/tex&quot;&gt;7\times 7&lt;/script&gt;的输入，那么padding在下面和右边都要加2才行，而且在局部结果拼最终结果时要舍去1行和1列。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;void winograd(float* input, float* kernel, float* output, int input_size)
{
    float **U;
    U = U_2x2_3x3(kernel);
    for (int l = 0; l &amp;lt; 2; ++l) {      //  H/m向上取整
        for (int n = 0; n &amp;lt; 2; ++n) {
            float **V, **Y;
            V = V_2x2_3x3(input, l * input_size * 2 + n * 2);
            Y = M_2x2_3x3(U, V);
            int row_col = l * 8 + n * 2;
            memcpy(output + row_col, Y[0], 2 * sizeof(float));
            memcpy(output + row_col + 4, Y[1], 2 * sizeof(float));

        }
    }
    for (int i = 0; i &amp;lt; 4; ++i) {
        free(U[i]);
    }
    free(U);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;剩余代码&quot;&gt;剩余代码&lt;/h2&gt;

&lt;p&gt;全部代码都在这里了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;void mm(float* input, float* kernel, float* output, int input_size, int kernel_size)
{
    int output_size = input_size - kernel_size + 1;
    for (int i = 0; i &amp;lt; output_size; ++i) {
        for (int j = 0; j &amp;lt; output_size; ++j) {
            float temp = 0;
            for (int k = 0; k &amp;lt; kernel_size; ++k) {
                for (int l = 0; l &amp;lt; kernel_size; ++l) {
                    temp += kernel[k*kernel_size+l] * input[i*input_size+j+k*input_size+l];
                }
            }
            output[i*output_size + j] = temp;
        }
    }
}

int main() {
    int input_size, kernel_size;
    input_size = 4;
    kernel_size = 3;
    float *input, *kernel, *output, *winograd_out;
    input = (float*)malloc(input_size * input_size * sizeof(float));
    for (int i = 0; i &amp;lt; input_size*input_size; ++i) {
        input[i] = i;
    }

    input = add_padding(input,input_size);
    input_size += 2;

    kernel = (float*)malloc(kernel_size * kernel_size * sizeof(float));
    for (int i = 0; i &amp;lt; kernel_size*kernel_size; ++i) {
        kernel[i] = 1;
    }

    int output_size = input_size;
    output = (float*)malloc(output_size * output_size * sizeof(float));
    memset(output, 0, output_size * output_size * sizeof(float));
    winograd_out = (float*)malloc(output_size * output_size * sizeof(float));
    memset(winograd_out, 0, output_size * output_size * sizeof(float));
    clock_t start, end1, end2;

    start = clock();
    mm(input, kernel, output, input_size, kernel_size);

    end1 = clock();
    winograd(input, kernel, winograd_out, input_size);
    end2 = clock();

    printf(&quot;mm cost %f , winograd cost %f&quot;, double(end1-start)/CLOCKS_PER_SEC, double(end2-end1)/CLOCKS_PER_SEC);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;最后&quot;&gt;最后&lt;/h3&gt;

&lt;p&gt;虽然结果是一样的，但是我实现的winograd比寻常卷积要慢好多╮(￣▽￣””)╭。具体原因应该还要我接下来摸索，应该可以考虑内存池优化、汇编优化、代码消冗余等。&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Jun 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/06/winograd2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/06/winograd2/</guid>
        
        <category>深度学习</category>
        
        <category>性能优化</category>
        
        
      </item>
    
      <item>
        <title>Winograd for CNN（一）</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;虽然现在深度卷积神经网络在计算机视觉领域表现的非常优秀，但它在大型数据集上训练时需要花费大量GPU计算时间，并且前向推理需要大量的计算力。我们希望深度卷积网络可以在嵌入式平台部署，并且希望在保证精度的情况下加快它的推理速度。常规的基于FFT的卷积对于大型滤波器是快速的，但是现有技术的卷积神经网络一般使用小的3×3滤波器。论文引入了基于Winograd的最小滤波算法，一种新的卷积神经网络快速算法。算法在小卷积上计算复杂度最小，这使得它在滤波器和batch小的情况下更快。论文使用VGG网络对算法的GPU实现进行基准测试，并展示了批处理大小从1到64的时时吞吐量。[1]&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cong和Xiao使用Strassen算法进行快速矩阵乘法，以减少卷积网络层中的调度次数，从而降低其总算术复杂度。 作者还提出，来自算术复杂性理论的更多技术可能适用于衔接。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;原始的Winograd算法，前置了很多数论方面的知识，为了效率我就没有深入的去阅读了。本文主要针对阅读了&lt;code class=&quot;highlighter-rouge&quot;&gt;Fast Algorithms for Convolutional Neural Networks&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;卷积公式&quot;&gt;卷积公式&lt;/h3&gt;

&lt;p&gt;假设卷积为G，图像为D，输入参数数量N，通道C，高H，宽W &lt;br /&gt;
卷积核参数通道C，高R，宽S，则卷积公式如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex1.png&quot; alt=&quot;&quot; height=&quot;40%&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以将整个图像的输出写作（其中*指代2D相关性）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex2.png&quot; alt=&quot;&quot; height=&quot;20%&quot; width=&quot;20%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;算法&quot;&gt;算法&lt;/h3&gt;

&lt;p&gt;假设用长度为r的FIR滤波器来得到输出m的式子为&lt;script type=&quot;math/tex&quot;&gt;F(m, r)&lt;/script&gt;，传统的winograd算法需要&lt;script type=&quot;math/tex&quot;&gt;µ(F(m,r)) = m + r - 1&lt;/script&gt;次乘法。我们可以通过堆叠一维算法来得到二维的最小算法——假设&lt;script type=&quot;math/tex&quot;&gt;F(m\times n, r\times s)&lt;/script&gt;指代用&lt;script type=&quot;math/tex&quot;&gt;r\times s&lt;/script&gt;的滤波器来计算得到&lt;script type=&quot;math/tex&quot;&gt;m\times n&lt;/script&gt;的输出，则它需要
&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex3.png&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
次乘法。以此为例，我们可以继续堆叠一维算法来得到多维的最小算法。&lt;br /&gt;
&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;但是需要注意不管是一维、二维还是多维的最快计算法，它要求输入的数量与所需乘法数一样。&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;f2times-23times-3&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2,3\times 3)&lt;/script&gt;&lt;/h2&gt;

&lt;p&gt;我们知道，乘法和加法在硬件实现上的时间复杂度一般是不一样的，乘法运算所需的时间通常远大于加法所需的时间。因此，用廉价运算代替昂贵运算也是加速运算的一种方法。原始的矩阵运算对于&lt;script type=&quot;math/tex&quot;&gt;F(2,3)&lt;/script&gt;需要6次乘法，而Winograd提出了如下算法，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex4.png&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
其中，
&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex5.png&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该算法只用了&lt;script type=&quot;math/tex&quot;&gt;2+3-1=4&lt;/script&gt;个乘法就计算得到了&lt;script type=&quot;math/tex&quot;&gt;F(2,3)&lt;/script&gt;，不过它涉及了4个与输入数据有关的加法，还有与常数滤波器有关的3个加法（&lt;script type=&quot;math/tex&quot;&gt;g_0+g_2&lt;/script&gt;只要算一次就行了）和2个乘法（因为滤波器为常数，所以这3个加法和2个乘法可以认为不占用时间）。&lt;/p&gt;

&lt;p&gt;我们可以将矩阵公式写成
&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex6.png&quot; alt=&quot;&quot; height=&quot;30%&quot; width=&quot;30%&quot; /&gt;
其中，&lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt;指逐元素的乘法（就是点乘，卷积用的）。对于&lt;script type=&quot;math/tex&quot;&gt;F(2,3)&lt;/script&gt;而言，上述公式各元素表示的意义如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex7.png&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;堆叠一维算法可以得到二维算法&lt;script type=&quot;math/tex&quot;&gt;F(m\times m, r\times r)&lt;/script&gt;如下（这一步论文没有具体的分析，不是很懂为什么，估计是各种线性变换(￣▽￣) ）
&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex8.png&quot; alt=&quot;&quot; height=&quot;30%&quot; width=&quot;30%&quot; /&gt;
其中，&lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt;是一个&lt;script type=&quot;math/tex&quot;&gt;r\times r&lt;/script&gt;的滤波器，&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;是一个&lt;script type=&quot;math/tex&quot;&gt;(m+r-1)\times(m+r-1)&lt;/script&gt;的输入图像块。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2, 3\times 3)&lt;/script&gt;用winograd只需要&lt;script type=&quot;math/tex&quot;&gt;4\times 4=16&lt;/script&gt;次乘法，而原始矩阵运算则需要&lt;script type=&quot;math/tex&quot;&gt;2\times 2\times 3\times 3=36&lt;/script&gt;次乘法运算。尽管winograd法还需要用32次加法来进行数据转换，用28个浮点运算指令来进行滤波器转换，用24次加法来进行反转变换，但是相比原始矩阵运算法还是提升很大。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2,3\times 3)&lt;/script&gt;可以被用来计算卷积核为&lt;script type=&quot;math/tex&quot;&gt;r\times r&lt;/script&gt;的卷积操作。其中，输入图像的每个通道需要被切割成&lt;script type=&quot;math/tex&quot;&gt;(m+r-1)\times(m+r-1)&lt;/script&gt;大小的块（每个块与相邻块间有&lt;script type=&quot;math/tex&quot;&gt;r-1&lt;/script&gt;的重叠区域），则每个通道可以有&lt;script type=&quot;math/tex&quot;&gt;P=\left\lceil H/m\right\rceil\times\left\lceil W/m\right\rceil&lt;/script&gt;个块。然后&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2,3\times 3)&lt;/script&gt;可以分别计算所有块然后累加得到最终结果。&lt;/p&gt;

&lt;p&gt;假设&lt;script type=&quot;math/tex&quot;&gt;U=G_gG^T&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;V=B^{T}dB&lt;/script&gt;，则
&lt;script type=&quot;math/tex&quot;&gt;Y=A^{T}[U\odot V]A&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;以&lt;script type=&quot;math/tex&quot;&gt;(\widetilde {x},\widetilde {y})&lt;/script&gt;为各个块坐标，&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;指单张图片，&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;为滤波器，则可以将上述卷积公式改写成，
&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex11.png&quot; alt=&quot;&quot; height=&quot;40%&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以下是具体实现的伪代码&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex12.png&quot; alt=&quot;&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文接下来的算法介绍主要提供了&lt;script type=&quot;math/tex&quot;&gt;F(3\times 3,2\times 2)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;F(4\times 4,3\times 3)&lt;/script&gt;的&lt;script type=&quot;math/tex&quot;&gt;A,G,B&lt;/script&gt;矩阵。&lt;/p&gt;

&lt;h3 id=&quot;理解&quot;&gt;理解&lt;/h3&gt;

&lt;p&gt;下面这个公式是最重要的一块。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2018-05-31-Winograd/tex8.png&quot; alt=&quot;&quot; height=&quot;30%&quot; width=&quot;30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A,G,B&lt;/script&gt;根据不同的卷积核有不同的值，而且是提前计算好的，可以用&lt;a href=&quot;https://github.com/andravin/wincnn&quot;&gt;https://github.com/andravin/wincnn&lt;/a&gt;的脚本计算。不过，&lt;script type=&quot;math/tex&quot;&gt;A,G,B&lt;/script&gt;也不是可以通过脚本直接得到的，还需要自己确定&lt;script type=&quot;math/tex&quot;&gt;m+r-2&lt;/script&gt;个插值点，wincnn的作者推荐了&lt;a href=&quot;https://openreview.net/forum?id=H1ZaRZVKg&amp;amp;noteId=H1ZaRZVKg&quot;&gt;https://openreview.net/forum?id=H1ZaRZVKg&amp;amp;noteId=H1ZaRZVKg&lt;/a&gt;可以帮助确定插值点。（感觉好难啊（−＿−；））&lt;/p&gt;

&lt;p&gt;自己推了好久，发现根本就没办法算，之后只能找代码看（&lt;a href=&quot;https://github.com/NervanaSystems/neon/blob/master/neon/backends/winograd.py&quot;&gt;winograd.py&lt;/a&gt;），原来winograd是一定要加padding补全的，加多少padding视情况而定。所以其实这个算法就是针对卷积套公式，不过卷积计算选择不同算法，速度会不太一样。（现在有许多版本的winograd）&lt;/p&gt;

&lt;p&gt;对于不同的输入用不同的padding补全，最后会选择性地舍弃部分数据。如&lt;script type=&quot;math/tex&quot;&gt;7\times 7&lt;/script&gt;的输入，在用&lt;script type=&quot;math/tex&quot;&gt;F(2\times 2, 3\times 3)&lt;/script&gt;计算时，会在左边补一个padding，在右边补两个padding，最后winograd卷积得到一个&lt;script type=&quot;math/tex&quot;&gt;8\times 8&lt;/script&gt;的输出，这时就要舍弃最右边的一列。&lt;/p&gt;

&lt;p&gt;我尝试写了一下自己版本的winograd算法，以便后面学习优化，具体可以看&lt;a href=&quot;/2018/06/winograd2&quot;&gt;下一篇&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;h5 id=&quot;1-lavin-a-gray-s-fast-algorithms-for-convolutional-neural-networkscproceedings-of-the-ieee-conference-on-computer-vision-and-pattern-recognition-2016-4013-4021&quot;&gt;1. Lavin A, Gray S. &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lavin_Fast_Algorithms_for_CVPR_2016_paper.pdf&quot;&gt;Fast algorithms for convolutional neural networks[C]&lt;/a&gt;//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4013-4021.&lt;/h5&gt;

&lt;h5 id=&quot;2-winograd-方法快速计算卷积&quot;&gt;2. &lt;a href=&quot;http://shuokay.com/2018/02/21/winograd/&quot;&gt;Winograd 方法快速计算卷积&lt;/a&gt;&lt;/h5&gt;

&lt;h5 id=&quot;3-知乎如何通俗易懂地解释卷积&quot;&gt;3. &lt;a href=&quot;https://www.zhihu.com/question/22298352&quot;&gt;知乎，如何通俗易懂地解释卷积？&lt;/a&gt;&lt;/h5&gt;
</description>
        <pubDate>Thu, 31 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/winograd/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/winograd/</guid>
        
        <category>深度学习</category>
        
        <category>性能优化</category>
        
        
      </item>
    
  </channel>
</rss>
